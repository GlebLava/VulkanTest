Ã¶Ray Tracing example: https://github.com/SaschaWillems/Vulkan/blob/master/examples/computeraytracing/computeraytracing.cpp
Fast Compute Shader example: https://bakedbits.dev/posts/vulkan-compute-example/


Vulkan steps with GLFW:

glfw init needs to be called before anything


create an Vulkan Instance:
Here is where we create our VulkanInstance. This is more or less the a context in openGL. This instance gives us all further means
and access to Vulkan as a library or program.

create a Surface:
Here we first need to create a window using any prefered way f.e. glfw. 
This window will act as a surface for vulkan so the connection there needs to be created as well.
(glfwCreateWindowSurface)

pick a physical device:
Physical devices are the actual GPUs. These need to be queried for their capabilities and supported features.
One can query them using vkEnumeratePhysicalDevices. The result is an array of VkPhysicalDevice. 
One of these is then to be picked to create a logical device from

create a logical device:
To be able to communicate with the picked physical device, a "logical" device needs to be created which is a VkDevice.
This VkDevice needs to be created with a DeviceQueue. 
What is a DeviceQueue?
Almost every operation in Vulkan, anything from drawing to uploading textures, requires commands to be submitted to a queue.
There are different types of queues that originate from different queue families and each family allows only a subset of command.
It needs to be checked which queue families are supported by the device for our desires.  

If the device supports all wanted queueFamilies, we need to pass the index of the wanted to queues inside of a array of VkDeviceQueueCreateInfo
to our VkDeviceCreateInfo. This also takes a VkPhysicalDeviceFeatures which can enable some specific device features? (Idk not used).
This VkDeviceCreateInfo is used to create the logical device.

It can be, depending on the requirements that a single queue can fullfill multiple requirements at once. So there only needs to be one
queue to the device. This is important to understand. 
For example for RayTracing I want a graphics, compute and present queue. It is possible that my device has one queue that can fullfill
all these roles, so at the end we need to only create one queue. 




create the swap chain:
Vulkan does not have the concept of a "default framebuffer", hence it requires an infrastructure that will own the buffers we will
render to before we visualize them on the screen. This infrastructure is known as the swap chain and must be created explicitly in Vulkan.
The swap chain is essentially a queue of images that are waiting to be presented to the screen. Our application will acquire such an 
image to draw to it, and then return it to the queue. How exactly the queue works and the conditions for presenting an image from the 
queue depend on how the swap chain is set up, but the general purpose of the swap chain is to synchronize the presentation of images with 
the refresh rate of the screen

Of course the device needs to support a swap chain. Also the swap chain is one of the device extensions, so it needs to be enabled 
in the creation of the logical device.
More details need to be queried as well:
-Basic surface capabilities (min/max number of images in swap chain, min/max width and height of images)
-Surface formats (pixel format, color space)
-Available presentation modes
Afterwards the right swap chain needs to be specified and created.
From this swap chain we get the swap chain images.

create the image views:
To use any VkImage, including those in the swap chain, in the render pipeline we have to create a VkImageView object. 
An image view is quite literally a view into an image. It describes how to access the image and which part of the image to 
access, for example if it should be treated as a 2D texture depth texture without any mipmapping levels

for each swap chain image, a swap chain image view needs to be created. 


create the render pass: https://www.youtube.com/watch?v=x2SGVjlVGhE
Here colorAttachments, subpasses and dependencies need to be configured to create the render pass. 
The renderpass is somewhat of a high-level description of a sequence of rendering operations. 
It defines attachments (like color and depth or stencil buffers) that will be used during rendering. 
A renderpass consists of one or more subpasses. Each subpass represents a distinct phase of the rendering process (color rendering, depth testing, etc)
Subpasses can read from and write to these attachments definded. 
The dependencies describe the ordering and synchronization between subpasses.


create the graphics pipeline:
Here we load in our shaders and define the layout of our graphics pipeline.

create the framebuffers:
Framebuffers bind our attachments (see create the render pass) that serve as the render targets for a specific render pass instance.
These are created based on the render pass definition and contain the attachments required for rendering. 

creating command pool and command buffers:
Commands in Vulkan, like drawing operations and memory transfers, are not executed directly using function calls. 
You have to record all of the operations you want to perform in command buffer objects. The advantage of this is that when we 
are ready to tell the Vulkan what we want to do, all of the commands are submitted together and Vulkan can more efficiently process 
the commands since all of them are available together. In addition, this allows command recording to happen in multiple threads if 
so desired

command pools manage the memory that is used to store the buffers and command buffers are allocated from them.
Command buffers are executed by submitting them on one of the device queues. 


#########################--Stuff--########################################
####### Resources and Descriptors:

Four fundamentally different types of resources: Buffers, Images, Samplers, Acceleration structures
Descriptors describe where to find a resource and how to access it (usage type, offsets, meta data)

Buffer types:
uniform buffer: (read/"load" only per draw call. That a uniform can not be changed during the draw call)

storage buffers: (read and write/"load and store")

texel buffers: (as uniform or storage) 
Here multiple values can be formed into one texel. 
Write and read operations of one texel are then atomic. (For example: 3floats can be formed into one texel)

dynamic buffers: (as uniform or storage)

Image types:
storage image: (can be written and read from using pixel coords)

sampled image: (can be written and read from but using normalised coords in the range from 0-1)

input attachment image: (read/"load" only)
can only be used within the renderpass and is framebuffer local only. That mean that the shader only
has access to ONE coordinate and not the neighbouring ones.


Image are (almost) always used through an image view. 
An image view is a wrapper around a vk::Image instance. 
An imageview is created with a subresourceRange. This is used if an imageView only can use a part or a layer
of an image. So multiple ImageViews can be created from one image, all used to access different part of the image.


- One descriptor describes exactly one resource
- Descriptors are always organized in descriptor sets
	Each set contains one or multiple descriptors which are used conjunction.
- Multiple descriptor sets can be used

- !Important! Descriptor sets are always bound do an Command Buffer



################# Commands and Command Buffers:
Commands are sent to the gpu over command buffers and not independenlty.
A command buffer needs to be bound to a command pool.

What is a command?
Commands are from different groups.
Action type commands and State type commands.
They are predefined. Take f.e. vkCmdDraw or vkCmdClearAttachments.

Commands are bound to descriptor sets. The descriptor sets "describe" which resources the command acts upon.


Every Command Buffer is created from a commandpool. A command pool must be created for a certain queue family. 
Afterswards a command buffer needs to be allocated from this command pool. 

In the render loop or whatever the commandBuffer then needs to be submitted with vk::QueueSubmit to the device for processing.
Thta where snchronization comes in, because submitting a command on the cpu side, does not wait for the command to finish on the gpu.
Code on the cpu will continue if synchronization is not used.

NOTE: It is normal to NOT reuse command buffers. They are typically allocated every render iteration from a new. The command pool should be 
reused tho. 
There are three command buffer usage modes: reuse (command buffer only allocated once), single use (command buffer allocated anew every time), reset and re-record

How can data be actually provided to commands?:
1. Descriptors establish links to resources
2. Push constants are a very small amount of data that can be stored along side with the commands directly in the command buffer (max 128 Bytes)
   (see Vulkan series 4 28:40)
3. Parameters of the actual commands (depends on the command)
4. Attributes (Graphics pipelines only)
   These are streamed to vertex shaders and are acessible via input location.  
   (see Vulkan series 4 30:45)




#########################--Workflow rewritten--###################################
INIT:
1. pick a physical device. 
2. create a logical device, with the appropiate queues.
3. Create a swapchain from the following modes (immeadiate, FIFO, FIFO relaxed, Mailbox) with a surface.
4. pick imageformats and create images.
5. create imageframes for each image.
	imageframe contains:
	- image created before?
	- imageview
	- buffer
	- fence
	- semaphore for when image is available
 	- semaphore for when image is finished being rendered to screen?
	- Descriptors, Descriprotsets, WriteDescriptorsets of Image 
6. create the pipeline














